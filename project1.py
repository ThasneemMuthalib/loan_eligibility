# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wXB2NqpfdRLiF_e5hiLhuSBUGoloyING

# Importing libraries
"""

import numpy as np        # To perform the Mathematical operation
import pandas as pd       # Data Manipulation tool

# Data Visualisation tool
import matplotlib.pyplot as plt       
import seaborn as sns                 

# To check accuracy
from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error

# For encoding and scaling 
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler

# train_test_split <- divide train and test set
# GridSearchCV <- classifier hyper parameter tunning 
from sklearn.model_selection import train_test_split, GridSearchCV

# Classification methods 
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC                         # Support vector classifier 
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

"""# Importing the datasets"""

train = pd.read_csv('/content/Copy of Training Data.csv')
test = pd.read_csv('/content/Copy of Testing Data.csv')

"""**Variables**

* Loan_ID----------------> Unique Loan ID.
* Gender ----------------> Male/ Female
* Married ---------------> Applicant married (Y/N)
* Dependents ------------> Number of dependents
* Education -------------> Applicant Education (Graduate/ Under Graduate)
* Self_Employed ---------> Self-employed (Y/N)
* ApplicantIncome -------> Applicant income
* CoapplicantIncome -----> Coapplicant income
* LoanAmount ------------> Loan amount in thousands
* Loan_Amount_Term ------> Term of a loan in months
* Credit_History --------> Credit history meets guidelines
* Property_Area ---------> Urban/ Semi-Urban/ Rural
* Loan_Status -----------> Loan approved (Y/N)
"""

train.head()

train.info()

test.head()

test.info()

train.isnull().sum()
# There are null values

test.isnull().sum()

"""**Missing values in training set**
* Credit_History 50
* Self_Employed 32
* LoanAmount 22
* Dependents 15
* Loan_Amount_Term 14
* Gender 13
* Married 3

#Descriptive analysis

##Data visualization

*1. Applicant income*
"""

sns.boxplot(x = train.ApplicantIncome)

"""**Applicant Income has outliers and it is right skewed. Boxplots cannot be observed clearly due to their high range. So, in order to sort it out, log transformation can be used on ApplicantIncome variable to shrink the range**"""

sns.boxplot(x = np.log(train.ApplicantIncome))

"""**Even after transforming, there are outliers to be seen. But towards symmetric**

*2. Coapplicant income*
"""

sns.boxplot(x = train.CoapplicantIncome)

"""**CoapplicantIncome is also right skewed and with clear outliers.**"""

sns.boxplot(x = np.log(train.CoapplicantIncome))

"""**Log transformation does not work.**

*3. Loan Amount*
"""

sns.boxplot(x = train.LoanAmount)

"""**There are outliers and right skewed. So, try log transfering.**"""

sns.boxplot(x = np.log(train.LoanAmount))

"""**Transfered more towards normal after applying log transformation but yet with outliers.**

*4. Loan amount term*
"""

plt.hist(train.Loan_Amount_Term)

"""*5. Correlation between numerical variables*"""

plt.figure(figsize=(7, 7))
sns.heatmap(train.corr(), annot=True, cmap='seismic', fmt='.2f')

"""**Applicant income is strongly related with the Loan amount compared to other variables.**"""

# sns.boxplot(x = train['Loan_Status'], y = train.ApplicantIncome)

# Log transfered ApplicantIncome
# sns.boxplot(x = train['Loan_Status'], y = np.log(train.ApplicantIncome))

# sns.boxplot(x = train['Loan_Status'], y = train.LoanAmount)

# Log transfered LoanAmount
# sns.boxplot(x = train['Loan_Status'], y = np.log(train.LoanAmount))

"""##Q1.
1. What's the ratio of Male to Female?

"""

train['Gender'].value_counts()

print(train['Gender'].value_counts()[0], ':', train['Gender'].value_counts()[1])

"""##Q2.
2. How many Males are married and non-married?
"""

train[train['Gender'] == 'Male']['Married'].value_counts()

print('Married male', train[train['Gender'] == 'Male']['Married'].value_counts()[0])
print('Unmarried male', train[train['Gender'] == 'Male']['Married'].value_counts()[1])

"""##Q3.
3. How many Females are married and non-married?

"""

train[train['Gender'] == 'Female']['Married'].value_counts()

print('Married female', train[train['Gender'] == 'Female']['Married'].value_counts()[1])
print('Unmarried female', train[train['Gender'] == 'Female']['Married'].value_counts()[0])

"""##Q4.
4. How many Males and Females are graduated and non-graduated with respect to their Marital status?

"""

train[train['Married'] == 'Yes'][train['Gender'] == 'Female']['Education'].value_counts()

"""**There are 25 female married graduates and 6 female married not graduates**"""

train[train['Married'] == 'No'][train['Gender'] == 'Female']['Education'].value_counts()

"""**There are 66 female unmarried graduates and 14 unmarried not graduates**"""

train[train['Married'] == 'Yes'][train['Gender'] == 'Male']['Education'].value_counts()

"""**There are 275 married male graduates and 82 married male not graduates**"""

train[train['Married'] == 'No'][train['Gender'] == 'Male']['Education'].value_counts()

"""**There are 99 unmarried male graduates and 31 unmarried male not graduates**

##Q6.
6. Is it a male or a female whose loan amount is the highest?
"""

train.loc[train.LoanAmount == train.LoanAmount.max(), :]

"""**Gender not mentioned of the highest LoanAMount taker**

##Q7.
7. the specification and information about the applicant who has the highest income?
"""

train.loc[train.ApplicantIncome == train.ApplicantIncome.max(), :]

"""**The highest income person is a married graduate male with 3+ dependents. He is not self employed and property area is rural. He has the highest income 81000 but credit history doesn't meet the guidelines (0 value) and with loan status No, Loan has not been approved.**

Relationship between Loan status and Credit history
"""

train[train['Credit_History'] == 1]['Loan_Status'].value_counts()

train[train['Credit_History'] == 0]['Loan_Status'].value_counts()

"""**Most of the loans have been approved when the credit history has met the guidelines. And diproved when the credit history has not met the guidelines.**

##Data cleaning

###**Credit_History NaN values imputation**
"""

train[train['Credit_History'].isnull()]

train['Credit_History'].value_counts()
# there are more 1's compared to 0's. so substitute the NaN's with 1.

train.Credit_History.fillna(1, inplace=True)

train['Credit_History'].value_counts()

train.Credit_History.isnull().sum()

test.Credit_History.fillna(1, inplace=True)

test.Credit_History.isnull().sum()

"""###**Self_Employed NaN values imputation**"""

train['Self_Employed'].value_counts()
# can substitute by 'No'

train.Self_Employed.fillna('No', inplace=True)
train['Self_Employed'].value_counts()

train.Self_Employed.isnull().sum()

test['Self_Employed'].value_counts()

test.Self_Employed.fillna('No', inplace=True)
test['Self_Employed'].value_counts()

test.Self_Employed.isnull().sum()

"""###**LoanAmount NaN values imputation**"""

train[train['LoanAmount'].isnull()]

train.LoanAmount.mode()

train.LoanAmount.mean()

train.LoanAmount.median()

"""**LoanAmount has outliers (shown before). Therefore, it is suitable to use median for missing value imputation**"""

train.LoanAmount.fillna(train.LoanAmount.median(), inplace=True)

train.LoanAmount.isnull().sum()

train.LoanAmount.value_counts()

test.LoanAmount.isnull().sum()

"""**Impute the missing values of test set LoanAmount using the median of training LoanAmount**"""

test.LoanAmount.fillna(train.LoanAmount.median(), inplace=True)

test.LoanAmount.isnull().sum()

"""### Dependents NaN values imputation"""

train[train.Dependents.isnull()]

train.Dependents.value_counts()

"""**Most of the NaN Dependents are Male and not Self_Employed**"""

train[train['Gender'] == 'Male'][train['Self_Employed'] == 'No']['Dependents'].value_counts()

train[train['Gender'] == 'Female'][train['Self_Employed'] == 'No']['Dependents'].value_counts()

"""**Both Male and Female non Self Employes have 0 dependents. Substituted missing Dependents by 0.** """

train.Dependents.fillna('0', inplace=True)

train.Dependents.value_counts()

test.Dependents.isnull().sum()

"""**Test set too has missing values. Missing value imputation will be similar to the imputation in training set.**"""

test.Dependents.fillna('0', inplace=True)

test.Dependents.isnull().sum()

"""### Loan_Amount_Term NaN values imputation"""

train.Loan_Amount_Term.value_counts()

train[train['Loan_Amount_Term'].isnull()]

plt.hist(train.Loan_Amount_Term)

train.Loan_Amount_Term.mode()

train.Loan_Amount_Term.mean()

train.Loan_Amount_Term.median()

"""**The Null values of Loan_Amount_Term will be substituted using the median of train set Loan_Amount_Term variable.**"""

train.Loan_Amount_Term.fillna(train.Loan_Amount_Term.median(), inplace=True)
train.Loan_Amount_Term.value_counts()

"""**Test set missing value imputation will be similar to the train set imputation.**"""

test.Loan_Amount_Term.fillna(train.Loan_Amount_Term.median(), inplace=True)
test.Loan_Amount_Term.value_counts()

"""### Gender NaN values imputation"""

train.Gender.value_counts()

train[train['Gender'].isnull()]

train[train['Education']=='Graduate'][train['Married']=='Yes']

# Married and Graduate 
train[train['Education']=='Graduate'][train['Married']=='Yes']['Gender'].value_counts()

# Married and Not Graduate
train[train['Education']=='Not Graduate'][train['Married']=='Yes']['Gender'].value_counts()

# Not Married and Graduate 
train[train['Education']=='Graduate'][train['Married']=='No']['Gender'].value_counts()

"""**Most of the missing values under Gender are Married and Graduates and few Married Not Graduates and Not Married Graduates. There are more males than female in all these categories. Therefore missing values under Gender was imputed as 'Male'.**"""

train.Gender.fillna('Male', inplace=True)
train.Gender.value_counts()

test.Gender.isnull().sum()

"""**Test set Gender missing value imputation is also filled as 'Male'.**"""

test.Gender.fillna('Male', inplace=True)

"""### Married NaN values imputation"""

train[train['Married'].isnull()]

train[train['Gender']=='Male']['Married'].value_counts()

train[train['Gender']=='Male'][train['ApplicantIncome']>3000][train['ApplicantIncome']<5000]['Married'].value_counts()

"""**There are more Married men than Unmarried men with an income between 3000 and 5000.**"""

train[train['Gender']=='Female']['Married'].value_counts()

train[train['Gender']=='Female'][train['ApplicantIncome']>9000]['Married'].value_counts()

"""**There are more Unmarried Female than Married Female with Income greater than 9000**

**The 2 missing values for 'Married' variable, Male Gender will be filled as Yes and Female Gender as No**
"""

train.iloc[[104,228]] = train.iloc[[104,228]].fillna('Yes')
train.iloc[435] = train.iloc[435].fillna('No')

train[train['Married'].isnull()]

test[test.Married.isnull()]

"""**Test set Married column is free of missing values**"""

train.isnull().sum()

test.isnull().sum()

"""**Data set is free from NULL values**"""

train.info()

test.info()

train['Property_Area'].value_counts()

train['Loan_Status'].value_counts()

"""##Encoding

##Q5. 

Which type of encoding will be required to perform on the "Education" column?

**Because there are only 2 categories, Label encoding would be okay**

###Label encoding
"""

le = LabelEncoder()

train.Gender = le.fit_transform(train.Gender)
test.Gender = le.fit_transform(test.Gender)

train.Married = le.fit_transform(train.Married)
test.Married = le.fit_transform(test.Married)

train.Education = le.fit_transform(train.Education)
test.Education = le.fit_transform(test.Education)

train.Self_Employed = le.fit_transform(train.Self_Employed)
test.Self_Employed = le.fit_transform(test.Self_Employed)

train.Property_Area = le.fit_transform(train.Property_Area)
test.Property_Area = le.fit_transform(test.Property_Area)

train.Loan_Status = le.fit_transform(train.Loan_Status)

"""###Onehot encoding"""

train.Dependents.unique()

ohe = OneHotEncoder(sparse=False)

new_Dependents = ohe.fit_transform(train[['Dependents']])

new_Dependents

train[['0_Dependents', '1_Dependent', '2_Dependents', '3+_Dependents']] = new_Dependents
train.head(10)

new_Dependents_test = ohe.fit_transform(test[['Dependents']])
new_Dependents_test

test[['0_Dependents', '1_Dependent', '2_Dependents', '3+_Dependents']] = new_Dependents_test
test.head()

"""##Drop columns

* ID number is not important for the analysis. Therefore, Load_ID column can be removed. 
* 4 new columns were created for Dependents. So, the Dependents column can be removed. 
"""

train.drop(['Loan_ID', 'Dependents'], axis=1, inplace=True)

test.drop(['Loan_ID', 'Dependents'], axis=1, inplace=True)

train.head()

test.head()

train.info()

test.info()

"""#Data transformation and remove outliers

Log transformation reduces the range and outliers. 
"""

train.ApplicantIncome = np.log(train.ApplicantIncome + 2)
train.LoanAmount = np.log(train.LoanAmount + 2)

# 2 has been added to make sure that there will not be any issue in log transfering if ApplicantIncome or LoanAmount is 0. 
#   Because log(0) is not defined.

sns.boxplot(train.ApplicantIncome)

# outliers
train.ApplicantIncome.nsmallest(3)

# outliers
train.ApplicantIncome.nlargest(3)

# remove outliers because it affects the training 
train.drop([216, 468, 600, 409, 333, 171], axis=0, inplace=True)

# index should be reset back because rows were dropped
train = train.reset_index()

sns.boxplot(train.ApplicantIncome)

"""**Less outliers. All the outliers cannot be removed because we clearly do not know whether they are unusual points. Therefore, much deviated points were only removed.**"""

sns.boxplot(train.LoanAmount)

# outliers
train.LoanAmount.nsmallest(2)

# drop 2 outliers
train.drop([563, 14], axis=0, inplace=True)

train = train.reset_index()
train.head()

train.drop(['level_0', 'index'], axis=1, inplace=True)

# Resetted Train set without few outliers 
train.head()

train.shape

# Test set log transformation to test the model
test.ApplicantIncome = np.log(test.ApplicantIncome + 2)
test.LoanAmount = np.log(test.LoanAmount + 2)

"""##Feature selection"""

feature = ExtraTreesClassifier(random_state=42)

x = train.drop('Loan_Status', axis=1)
y = train['Loan_Status']

feature.fit(x,y)

feature.feature_importances_

important_features = pd.Series(feature.feature_importances_, index = x.columns)
important_features

important_features.nlargest(14).plot(kind='barh')

"""##Q8.
8. What all features are you going to consider for the model training and why? (Give the appropriate reason for 
choosing such features among others)

**Dependents are less important compared to other features. Therefore, 0, 1, 2 and 3+ dependents columns (original Dependent column) are dropped for further analysis**
"""

new_x = train[list(important_features.nlargest(10).index)]
new_x

"""##Model building"""

test.drop(['0_Dependents', '1_Dependent', '2_Dependents', '3+_Dependents'], axis=1, inplace=True)
test_features = ['Credit_History',	'ApplicantIncome'	,'LoanAmount'	,'CoapplicantIncome'	,'Property_Area',	'Loan_Amount_Term',
                 'Self_Employed','Education',	'Gender'	,	'Married']
test = test[test_features]

test.head()

x_train = new_x
y_train = y

x_test = test

#x_train, x_test, y_train, y_test = train_test_split(new_x, y, test_size=0.2, random_state=42)

"""###Naive Bayes Classifier"""

nb_model = GaussianNB()

# training phase 
nb_model.fit(x_train, y_train)

# predictions on test set
nb_test_pred = nb_model.predict(x_test)

# predictions on train set
nb_train_pred = nb_model.predict(x_train)

"""**Accuracy was checked using the training set because the actual test set target variable (y_test) is not given.**"""

# Accuracy of the Naive Bayes model using the train set
score_nb = nb_model.score(x_train, y_train)
print('accuracy :', score_nb)
nb = round(score_nb*100,2)
print('accuracy % :', nb)

"""###Support vector classifier"""

svc_model = SVC()

# training phase 
svc_model.fit(x_train, y_train)

# predictions on test set
svc_test_pred = svc_model.predict(x_test)

# predictions on train set
svc_train_pred = svc_model.predict(x_train)

# Accuracy of the Support vector classifier model using the train set
svc_accuracy = accuracy_score(y_train, svc_train_pred)
svc = round(svc_accuracy*100, 2)
svc

# svc = pd.DataFrame({'Actual':y_train, 'Predicted':svc_train_pred})
# sns.heatmap(svc.corr(), annot=True, cmap='Greens')

########## without considering xtest
# svc_accuracy = accuracy_score(y_test, svc_pred)
# svc_accuracy

"""###Logistic regression"""

lor_model = LogisticRegression()

# training phase 
lor_model.fit(x_train, y_train)

# predictions on test set
lor_test_pred = lor_model.predict(x_test)

# predictions on train set
lor_train_pred = lor_model.predict(x_train)

# Accuracy of the Logistic regression model using the train set
lor_accuracy = accuracy_score(y_train, lor_train_pred)
lor = round(lor_accuracy*100, 2)
lor

########## without considering xtest
# lor_accuracy = accuracy_score(y_test, lor_pred)
# lor_accuracy

# lor = pd.DataFrame({'Actual':y_train, 'Predicted':lor_train_pred})
# sns.heatmap(lor.corr(), annot=True, cmap='Greens')

"""###Decision tree classifier

##Q10.
10. Have you performed the Hyper Parameter Tuning?

**yes**

####Hyper parameter tunning
"""

# The function to measure the quality of a split
criterion = ["gini", "entropy"]

# The number of features to consider when looking for the best split
max_features = ['auto', 'sqrt']

# The maximum depth of the tree
max_depth = [i for i in range(5, 55, 5)]

# The minimum number of samples required to split an internal node
min_samples_split = [2, 5, 10, 30, 100]

# The minimum number of samples required to be at a leaf node
min_samples_leaf = [1, 2, 5, 10]

param_grid = {'criterion': criterion,
              'max_features': max_features,
              'max_depth': max_depth,
              'min_samples_split': min_samples_split,
              'min_samples_leaf': min_samples_leaf}
param_grid

dc_model = DecisionTreeClassifier(random_state = 42)

dc_gridCV = GridSearchCV(estimator = dc_model, param_grid = param_grid, cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

# training phase 
dc_gridCV.fit(x_train, y_train)

dc_gridCV.best_params_

##############################################################
# dc_gridCV = DecisionTreeClassifier(criterion='entropy', max_depth = 5, max_features = 'auto', min_samples_split = 5, 
#                                    min_samples_leaf = 1, random_state = 42)

##########################################
# dc_gridCV.fit(x_train, y_train)

# predictions on test set
dc_test_pred = dc_gridCV.predict(x_test)

# predictions on train set
dc_train_pred = dc_gridCV.predict(x_train)

# Accuracy of Decision tree classifier model using train set
dc_accuracy = accuracy_score(y_train, dc_train_pred)
dc = round(dc_accuracy*100, 2)
dc

########## without considering xtest
# dc_accuracy = accuracy_score(y_test, dc_pred)
# dc_accuracy

# dc = pd.DataFrame({'Actual':y_train, 'Predicted':dc_train_pred})
# sns.heatmap(dc.corr(), annot=True, cmap='Greens')

"""###Random forest classifier

####Hyper parameter tunning
"""

# The number of trees in the forest
n_estimators = [int(i) for i in range(100, 501, 100)]

# The number of features to consider when looking for the best split
max_features = ['auto', 'sqrt']

# The maximum depth of the tree
max_depth = [2, 3, 5, 10, 20]

# The minimum number of samples required to be at a leaf node
min_samples_leaf = [1, 2, 5, 10]

param_grid = {'n_estimators': n_estimators,
              'max_features': max_features,
              'max_depth': max_depth,
              'min_samples_leaf': min_samples_leaf}
param_grid

rf_model = RandomForestClassifier(random_state = 42)

rf_gridCV = GridSearchCV(estimator = rf_model, param_grid = param_grid, scoring = 'neg_mean_squared_error', n_jobs = 1, verbose=2, cv=5)

# training phase 
rf_gridCV.fit(x_train, y_train)

rf_gridCV.best_params_

#####################################################
# rf_gridCV = RandomForestClassifier(n_estimators = 100, max_depth = 2, min_samples_leaf = 1, max_features = 'auto', random_state = 42)

#######################################################
# rf_gridCV.fit(x_train, y_train)

# predictions on test set
rf_test_pred = rf_gridCV.predict(x_test)

# predictions on train set
rf_train_pred = rf_gridCV.predict(x_train)

# accuracy of Random forest classifier model using train set
rf_accuracy = accuracy_score(y_train, rf_train_pred)
rf = round(rf_accuracy*100, 2)
rf

# without considering test set 
# rf_accuracy = accuracy_score(y_test, rf_pred)
# rf_accuracy

# rf_df = pd.DataFrame({'Actual':y_train, 'Predicted':rf_train_pred})
# sns.heatmap(rf_df.corr(), annot=True, cmap='Greens')

"""##Q9.
9. Which model are you going to choose and define the reason for going with the same model?

* lor <- 81.35%
* rf <- 81.19%
* nb <- 80.69%
* dc <- 79.87%
* svc <- 69.47%

**Logistic regression model gives the best model with the highest accuracy of 81.35%. Overall all the models are giving an accuracy closer to 70%.**

##Q12.
12. What's the accuracy of your model?

> 0.8135313531353136

##Q11.
11. Compare the Actual results with the Predicted results

**The actual values of the target variable is not given in the test set. Therefore, the predicted test values cannot be compared with its actual ones.**

**Instead, the training set values can be compared.**
"""

# Actual Vs Predicted comparison
lor_df = pd.DataFrame({'Actual':y_train, 'Predicted':lor_train_pred})
lor_df.head(10)

# The predicted values of the test set
lor_test_pred

sns.heatmap(lor_df.corr(), annot=True, cmap='Greens')

"""**Correlation between actual and predicted is not very close to 1. But it gives a correlation of 0.55 (55%).**

##Q13.
13. In order to improve the accuracy, what more approaches can be done?

* Remove or substitute an appropriate value for outliers.
* Collect more data. Because 600+ data points are not sufficient to build an accurate model.
* Increasing the number of parameters in hyperparameter tunning.
"""

######################################################################################## fill

"""##Q14.
14. What can you conclude with respect to the data?

* Most of the loan status has been disproved when the credit history does not meet the guidelines.

#Pickle
"""

import pickle